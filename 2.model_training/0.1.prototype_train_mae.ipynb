{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook runs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import yaml\n",
    "import subprocess\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import umap.umap_ as umap\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import seaborn as sns\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Column Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_TYPE_COL = 'encode_celltype'\n",
    "SAMPLE_ID_COL = 'sample_id'\n",
    "STIM_COL = 'stim'\n",
    "\n",
    "GENE_ID_COL = 'gene_ids'\n",
    "\n",
    "DATASPLIT_COL = 'isTraining'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths to Required Input Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config\n",
    "The config file specifies the path to data and software repo (due to currently in active development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root directory of the analysis repository\n",
    "REPO_ROOT = subprocess.run(\n",
    "    [\"git\", \"rev-parse\", \"--show-toplevel\"], capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "REPO_ROOT = pathlib.Path(REPO_ROOT)\n",
    "\n",
    "CONFIG_FILE = REPO_ROOT / 'config.yml'\n",
    "assert CONFIG_FILE.exists(), f\"Config file not found at {CONFIG_FILE}\"\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as file:\n",
    "    config_dict = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buddi_fork_path = config_dict['software_path']['buddi_HGSC']\n",
    "buddi_fork_path = pathlib.Path(buddi_fork_path)\n",
    "assert buddi_fork_path.exists(), f\"buddi fork not found at {buddi_fork_path}\"\n",
    "\n",
    "sys.path.insert(0, str(buddi_fork_path))\n",
    "# this is quite ugly, once activate modifications are done this will be changed\n",
    "# to a proper installation + import\n",
    "from buddi.plotting import validation_plotting as vp\n",
    "from buddi.models.buddi4 import build_buddi4, fit_buddi4\n",
    "# from prototype_buddi_dataset import *\n",
    "from buddi.dataset.dataset import dataset_generator, dataset_generator_unsupervised, get_output_signature, get_output_signature_unsupervised\n",
    "from buddi.dataset.utils import split_dataset\n",
    "from buddi.plotting.plot_latent_space import plot_latent_spaces_buddi4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Pre-Processing Output presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_OUTPUT_PATH = REPO_ROOT / 'processed_data'\n",
    "assert PREPROCESSING_OUTPUT_PATH.exists(), f\"Preprocessing output path {PREPROCESSING_OUTPUT_PATH} does not exist\"\n",
    "\n",
    "SC_AUGMENTED_DATA_PATH = PREPROCESSING_OUTPUT_PATH / 'sc_augmented'\n",
    "assert SC_AUGMENTED_DATA_PATH.exists(), f\"Single cell augmented data path {SC_AUGMENTED_DATA_PATH} does not exist\"\n",
    "pseudobulk_files = list(SC_AUGMENTED_DATA_PATH.glob('*_splits.pkl'))\n",
    "assert len(pseudobulk_files) > 0, f\"No pseudobulk files found in {SC_AUGMENTED_DATA_PATH}\"\n",
    "\n",
    "BULK_FORMAT_DATA_PATH = PREPROCESSING_OUTPUT_PATH / 'bulk_formatted'\n",
    "assert BULK_FORMAT_DATA_PATH.exists(), f\"Bulk format data path {BULK_FORMAT_DATA_PATH} does not exist\"\n",
    "formatted_bulk_files = list(BULK_FORMAT_DATA_PATH.glob('*.h5ad'))\n",
    "assert len(formatted_bulk_files) > 0, f\"No formatted bulk files found in {BULK_FORMAT_DATA_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-coded path to Cibersortx signature gene file (TODO make this more elegant?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIBERSORTX_SIG_GENE_FILE = pathlib.Path(config_dict['data_path']['sc_data_path']) /\\\n",
    "    'GSE154600_cibersortx_output' /\\\n",
    "    'CIBERSORTx_Job4_GSM4675273_cibersortx_sc_reference_input_inferred_phenoclasses.CIBERSORTx_Job4_GSM4675273_cibersortx_sc_reference_input_inferred_refsample.bm.K999.txt'\n",
    "assert CIBERSORTX_SIG_GENE_FILE.exists(), f\"CIBERSORTx signature gene file not found at {CIBERSORTX_SIG_GENE_FILE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data and Feature select using cibersortx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cibersortx signature genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cibersortx_sig_df = pd.read_csv(CIBERSORTX_SIG_GENE_FILE, sep='\\t', header=0)\n",
    "cibersortx_sig_genes = cibersortx_sig_df['NAME'].values.tolist()\n",
    "print(f\"{len(cibersortx_sig_genes)} signature genes from CIBERSORTx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO support for multiple formatted bulk files\n",
    "processed_bulk = sc.read(formatted_bulk_files[0]) # Assumes only single file for processed bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_sample_ids = processed_bulk.obs[SAMPLE_ID_COL].unique()\n",
    "n_bulk_samples = len(bulk_sample_ids)\n",
    "print(f\"Number of bulk samples: {n_bulk_samples}\")\n",
    "\n",
    "bulk_stims = processed_bulk.obs[STIM_COL].unique()\n",
    "n_bulk_stims = len(bulk_stims)\n",
    "print(f\"Number of bulk stims: {n_bulk_stims}\")\n",
    "\n",
    "bulk_genes = processed_bulk.var[GENE_ID_COL].values.tolist()\n",
    "print(f\"Number of bulk genes: {len(bulk_genes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudobulks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_gene_file = SC_AUGMENTED_DATA_PATH.glob('*_genes.pkl')\n",
    "sc_gene_file = list(sc_gene_file)[0]\n",
    "sc_genes = pd.read_pickle(sc_gene_file)\n",
    "print(f\"Number of single cell genes: {len(sc_genes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the overlap of genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn3(\n",
    "    [\n",
    "        set(sc_genes),\n",
    "        set(bulk_genes),\n",
    "        set(cibersortx_sig_genes)\n",
    "    ],\n",
    "    set_labels=['Single Cell', 'Bulk', 'CIBERSORTx']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_bulk_intersection_genes = set(sc_genes).intersection(set(bulk_genes))\n",
    "sc_bulk_intersection_genes = list(sc_bulk_intersection_genes)\n",
    "cibersortx_sig_genes = set(cibersortx_sig_genes).intersection(sc_bulk_intersection_genes)\n",
    "cibersortx_sig_genes = list(cibersortx_sig_genes)\n",
    "\n",
    "print(f\"Number of genes in intersection of single cell and bulk: {len(sc_bulk_intersection_genes)}\")\n",
    "\n",
    "## Subset the bulk data to only include the intersection genes\n",
    "processed_bulk = processed_bulk[\n",
    "    :, \n",
    "    processed_bulk.var[GENE_ID_COL].isin(sc_bulk_intersection_genes)]\n",
    "\n",
    "X_bulk_train = pd.DataFrame(\n",
    "    processed_bulk.X,\n",
    "    index=processed_bulk.obs[SAMPLE_ID_COL],\n",
    "    columns=processed_bulk.var[GENE_ID_COL]\n",
    ")\n",
    "\n",
    "# No Y for real bulk\n",
    "\n",
    "# Add metadata columns \n",
    "## TODO: perhaps move this to preprocessing?\n",
    "meta_bulk_train = processed_bulk.obs.loc[:,[SAMPLE_ID_COL, STIM_COL]]\n",
    "meta_bulk_train['isTraining'] = 'Train'\n",
    "meta_bulk_train['cell_prop_type'] = 'bulk'\n",
    "meta_bulk_train['samp_type'] = 'bulk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pseudobulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^(.*)_((?:Train)|(?:Test))_((?:meta)|(?:prop)|(?:pseudo))_splits\\.pkl$')\n",
    "\n",
    "# Temporary dictionary to group by sample_id\n",
    "temp_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for path in pseudobulk_files:\n",
    "    file = path.name\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        sample_id, datasplit, datatype = match.groups()\n",
    "        temp_dict[datasplit][sample_id][datatype] = path\n",
    "\n",
    "# Organizing the sorted results\n",
    "grouped_files = {\n",
    "    \"Train\": {\"meta\": [], \"prop\": [], \"pseudo\": []},\n",
    "    \"Test\": {\"meta\": [], \"prop\": [], \"pseudo\": []}\n",
    "}\n",
    "\n",
    "# Sort within each datasplit by sample_id and organize the lists\n",
    "for datasplit in [\"Train\", \"Test\"]:\n",
    "    sorted_samples = sorted(temp_dict[datasplit].keys())  # Sort by sample_id\n",
    "    for sample_id in sorted_samples:\n",
    "        for datatype in [\"meta\", \"prop\", \"pseudo\"]:\n",
    "            if datatype in temp_dict[datasplit][sample_id]:\n",
    "                grouped_files[datasplit][datatype].append(temp_dict[datasplit][sample_id][datatype])\n",
    "\n",
    "meta_sc_train = pd.concat(\n",
    "    [pd.read_pickle(file) for file in grouped_files['Train']['meta']]\n",
    ")\n",
    "meta_sc_train['stim'] = 'white' # ugly fix\n",
    "Y_sc_train = pd.concat(\n",
    "    [pd.read_pickle(file) for file in grouped_files['Train']['prop']]\n",
    ")\n",
    "X_sc_train = pd.concat(\n",
    "    [pd.read_pickle(file).loc[:, sc_bulk_intersection_genes] for file in grouped_files['Train']['pseudo']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate X, Y and Encode Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate place holder for bulk proportion (not used)\n",
    "Y_bulk_dummy = pd.DataFrame(\n",
    "    np.zeros((X_bulk_train.shape[0], Y_sc_train.shape[1])),\n",
    "    columns=Y_sc_train.columns\n",
    ")\n",
    "\n",
    "X_concat = pd.concat([X_bulk_train, X_sc_train])\n",
    "Y_concat = pd.concat([Y_bulk_dummy, Y_sc_train])\n",
    "meta_concat = pd.concat([meta_bulk_train, meta_sc_train])\n",
    "\n",
    "# save gene and cell type names\n",
    "X_gene_names = X_concat.columns.to_list()\n",
    "Y_cell_type_names = Y_concat.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the top variable genes\n",
    "X_colmean = X_concat.values.mean(axis=0) # mean across samples\n",
    "X_colvar = X_concat.values.var(axis=0) # variance across samples\n",
    "# coefficient of variation which is the var to mean ratio\n",
    "X_CoV = np.array(np.divide(X_colvar, X_colmean+0.001)) \n",
    "\n",
    "# need to get the genes such that\n",
    "# the union of the highly variable and the\n",
    "# CIBERSORTx genes are 7000 total\n",
    "num_genes_found = False\n",
    "\n",
    "gene_df = pd.DataFrame(X_gene_names, columns=['gene'])\n",
    "\n",
    "# start with top 7000 genes by CoV\n",
    "initial_count = 7000\n",
    "while not num_genes_found:\n",
    "    # retrieve the top initial_count genes by CoV\n",
    "    idx_top = np.argpartition(X_CoV, -initial_count)[-initial_count:]\n",
    "    # get gene names from idx_top\n",
    "    top_gene_df = gene_df.iloc[idx_top]\n",
    "\n",
    "    # produce the union of the top genes and the cibersort genes\n",
    "    CoV_only = np.union1d(top_gene_df, cibersortx_sig_genes)\n",
    "\n",
    "    # check if the union is 7000\n",
    "    if len(CoV_only) == 7000:\n",
    "        num_genes_found = True\n",
    "    else:\n",
    "        # if not decrement the top CoV gene to retrieve by 1\n",
    "        # in the next iteration, there will be two possibilities\n",
    "        # 1. the union will have one less gene due to the removed gene being only in the CoV genes\n",
    "        # 2. the union will have the same number of genes as this iteration due to the removed \n",
    "        #    gene being also present in CIBERSORTx genes\n",
    "        # keep running this loop until the union has 7000 genes\n",
    "        initial_count = initial_count -1\n",
    "\n",
    "idx_top = np.argpartition(X_CoV, -initial_count)[-initial_count:] # num_genes to get 7000\n",
    "gene_df = gene_df.iloc[idx_top]\n",
    "venn2([set(gene_df.values.flatten()), \n",
    "       set(cibersortx_sig_genes)], \n",
    "       set_labels = ('Top CoV Genes', 'Cibersort Genes'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select_genes = np.union1d(gene_df, cibersortx_sig_genes)\n",
    "\n",
    "# to numpy matrix\n",
    "X = X_concat.loc[:,feature_select_genes].to_numpy()\n",
    "print(X.shape)\n",
    "\n",
    "Y = Y_concat.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS_TO_ENCODE = [SAMPLE_ID_COL, STIM_COL, 'samp_type']\n",
    "\n",
    "encoded = {}\n",
    "\n",
    "for field in FIELDS_TO_ENCODE:\n",
    "    encoder = OneHotEncoder(dtype=int)\n",
    "    encoded[field] = pd.DataFrame(\n",
    "        encoder.fit_transform(meta_concat.loc[:,[field]]).toarray(),\n",
    "        columns=encoder.get_feature_names_out([field])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize within sample\n",
    "clip_upper = np.quantile(X, 0.9)\n",
    "X_full = np.clip(X, 0, clip_upper)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_full)\n",
    "\n",
    "# now normalize with the scaler trained on the \n",
    "# training data\n",
    "X_full = np.clip(X_full, 0, clip_upper)\n",
    "X_full = scaler.transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = np.where(meta_concat.isTraining==\"Train\")[0] \n",
    "idx_bulk = np.where(meta_concat.samp_type == \"bulk\")[0]\n",
    "\n",
    "# for unknown proportions; \n",
    "# this is bulks used in training\n",
    "idx_bulk_train = np.intersect1d(idx_bulk, idx_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = np.where(meta_concat.isTraining==\"Train\")[0] \n",
    "idx_bulk = np.where(meta_concat.samp_type == \"bulk\")[0]\n",
    "idx_sc = np.where(meta_concat.samp_type != \"bulk\")[0]\n",
    "\n",
    "# for unknown proportions; \n",
    "# this is bulks used in training\n",
    "idx_bulk_train = np.intersect1d(idx_bulk, idx_train)\n",
    "np.random.shuffle(idx_bulk_train) # shuffle the indices randomly\n",
    "\n",
    "X_unkp = X_full[idx_bulk_train,]\n",
    "label_unkp = encoded['sample_id'].values[idx_bulk_train,]\n",
    "drug_unkp = encoded['stim'].values[idx_bulk_train,]\n",
    "bulk_unkp = encoded['samp_type'].values[idx_bulk_train,]\n",
    "y_unkp = Y[idx_bulk_train,]\n",
    "\n",
    "# for known proportions\n",
    "idx_sc_train = np.intersect1d(idx_sc, idx_train)\n",
    "np.random.shuffle(idx_sc_train) # shuffle the indices randomly\n",
    "\n",
    "X_kp = X_full[idx_sc_train,]\n",
    "label_kp = encoded['sample_id'].values[idx_sc_train,]\n",
    "drug_kp = encoded['stim'].values[idx_sc_train,]\n",
    "bulk_kp = encoded['samp_type'].values[idx_sc_train,]\n",
    "y_kp = Y[idx_sc_train,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below, we plot the pseudobulk and real bulk data. Each plot will have a different coloration in order to highlight the different confounders present in the data.\n",
    "In the first plot, it is colored by sample ID, but since there are >500 real bulk samples, we hide the legend.\n",
    "The final plot is colored by the cell-type that has the highest proportion in the pseudobulk. \n",
    "For the bulk data, this is a random cell-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## plot samples\n",
    "# plot_df = vp.get_tsne_for_plotting(X_full)\n",
    "\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(30,15))\n",
    "\n",
    "# Y_temp = np.copy(Y)\n",
    "# Y_temp = np.argmax(Y_temp, axis=1) \n",
    "\n",
    "# vp.plot_tsne(plot_df, meta_concat.sample_id.to_numpy(), axs[0,0], title=f\"\")\n",
    "# vp.plot_tsne(plot_df, meta_concat.stim.to_numpy(), axs[0,1], title=f\"\")\n",
    "# vp.plot_tsne(plot_df, meta_concat.isTraining.to_numpy(), axs[0,2], title=f\"\")\n",
    "# vp.plot_tsne(plot_df, meta_concat.cell_prop_type.to_numpy(), axs[1,0], title=f\"\")\n",
    "# vp.plot_tsne(plot_df, meta_concat.samp_type.to_numpy(), axs[1,1], title=f\"\")\n",
    "# vp.plot_tsne(plot_df, Y_temp, axs[1,2], title=f\"\")\n",
    "\n",
    "\n",
    "# fig.suptitle(\"All Data, normalized using training data\", fontsize=14)\n",
    "\n",
    "# axs[0,0].legend([],[], frameon=False)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = X_full.shape[1]\n",
    "n_y = Y.shape[1]\n",
    "n_labels = encoded['sample_id'].shape[1]\n",
    "n_stims = encoded['stim'].shape[1]\n",
    "n_samp_types = encoded['samp_type'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_buddi, unsupervised_buddi = build_buddi4(\n",
    "    n_x=n_x,\n",
    "    n_y=n_y,\n",
    "    n_labels=n_labels,\n",
    "    n_stims=n_stims,\n",
    "    n_samp_types=n_samp_types,\n",
    "    reconstr_loss_fn = tf.keras.losses.mean_squared_error, # default reconstruction loss is mean squared error\n",
    "    classifier_loss_fn = tf.keras.losses.mean_absolute_error, # default classifier loss is mean absolute error\n",
    "    label_classifier_loss_fn = tf.keras.losses.binary_crossentropy # only sample id classifier loss is binary crossentropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(supervised_buddi, to_file='supervised_buddi.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(unsupervised_buddi, to_file='unsupervised_buddi.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_supervised = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator(X_kp, y_kp, label_kp, drug_kp, bulk_kp),\n",
    "    output_signature=get_output_signature(X_kp, y_kp, label_kp, drug_kp, bulk_kp)\n",
    ")\n",
    "\n",
    "dataset_unsupervised = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_generator_unsupervised(X_unkp, label_unkp, drug_unkp, bulk_unkp),\n",
    "    output_signature=get_output_signature_unsupervised(X_unkp, label_unkp, drug_unkp, bulk_unkp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training pseudobulks\n",
    "idx_tmp_bulk = np.where(np.logical_and(meta_concat.isTraining == \"Train\", meta_concat.samp_type == \"sc_ref\"))[0]\n",
    "idx_tmp_bulk = np.random.choice(idx_tmp_bulk, 500, replace=True)\n",
    "\n",
    "# get the real bulks\n",
    "idx_tmp = np.where(np.logical_and(meta_concat.isTraining == \"Train\", meta_concat.samp_type == \"bulk\"))[0]\n",
    "idx_tmp = np.random.choice(idx_tmp, 500, replace=True)\n",
    "\n",
    "# concatenate so we have them all\n",
    "idx_tmp = np.concatenate((idx_tmp, idx_tmp_bulk))\n",
    "\n",
    "X_tmp = X_full[idx_tmp,]\n",
    "meta_tmp = meta_concat.iloc[idx_tmp,]\n",
    "\n",
    "cell_types = list(Y_concat.columns)\n",
    "Y_tmp = Y[idx_tmp,]\n",
    "idx_sc_prop = np.where(meta_tmp.cell_prop_type == \"single_celltype\")[0]\n",
    "cell_type_labels = [cell_types[i] for i in np.argmax(Y_tmp[idx_sc_prop, :], axis=1)]\n",
    "cell_type_col = meta_tmp['cell_prop_type'].values.copy()\n",
    "cell_type_col[idx_sc_prop] = cell_type_labels\n",
    "meta_tmp.loc[:,'cell_type'] = cell_type_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df_concat = pd.DataFrame()\n",
    "\n",
    "for _i in range(4):\n",
    "\n",
    "    all_loss_df = fit_buddi4(\n",
    "        supervised_buddi, unsupervised_buddi, \n",
    "        dataset_supervised, dataset_unsupervised, \n",
    "        epochs=50, batch_size=16, shuffle_cache_size=10000\n",
    "    )\n",
    "    loss_df_concat = pd.concat([loss_df_concat, all_loss_df], ignore_index=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(25, 5), sharex=True)\n",
    "\n",
    "    loss_columns = [\n",
    "        'X_reconstruction_loss', \n",
    "        'classifier_label_loss', \n",
    "        'classifier_stim_loss', \n",
    "        'classifier_samp_type_loss', \n",
    "        'prop_estimator_loss'\n",
    "    ]\n",
    "\n",
    "    for ax, col in zip(axes, loss_columns):\n",
    "        sns.lineplot(\n",
    "            data=all_loss_df, \n",
    "            x='epoch', \n",
    "            y=col, \n",
    "            hue='type', \n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plot_latent_spaces_buddi4(\n",
    "        unsupervised_buddi,\n",
    "        X_tmp,\n",
    "        meta_tmp,\n",
    "        type='UMAP',\n",
    "        alpha=1,    \n",
    "        panel_width=3\n",
    "    )\n",
    "\n",
    "    plot_latent_spaces_buddi4(\n",
    "        unsupervised_buddi,\n",
    "        X_tmp,\n",
    "        meta_tmp,\n",
    "        type='PCA',\n",
    "        alpha=1,    \n",
    "        panel_width=3\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_scanpy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
